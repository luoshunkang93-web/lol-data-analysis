import requests
import pandas as pd
import sqlite3
import time
import random
from datetime import datetime  # <--- [å˜åŒ–1] å¼•å…¥æ—¶é—´åº“

# 1. åˆå§‹åŒ–æ•°æ®åº“
conn = sqlite3.connect('lol_analysis.db')
print(f"ðŸš€ [Backend] Starting Data Pipeline at {datetime.now()}...")

# ==========================================
# ðŸ“¦ æ­¥éª¤ A: èŽ·å– Riot è‹±é›„æ•°æ® (ä¸»è¡¨)
# ==========================================
print("\nðŸ“¥ [1/2] Fetching Riot Champion Data...")
try:
    # èŽ·å–æœ€æ–°ç‰ˆæœ¬å·
    VERSION_URL = "https://ddragon.leagueoflegends.com/api/versions.json"
    latest_version = requests.get(VERSION_URL).json()[0]
    
    # èŽ·å–è‹±é›„æ•°æ®
    CHAMP_URL = f"https://ddragon.leagueoflegends.com/cdn/{latest_version}/data/zh_CN/champion.json"
    response = requests.get(CHAMP_URL)
    champ_data = response.json()['data']

    difficulty_list = []
    target_champs = [] 
    
    # èŽ·å–ä»Šå¤©çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "2024-02-18"
    today_str = datetime.now().strftime("%Y-%m-%d")

    for en_name, data in champ_data.items():
        difficulty_list.append({
            "Champion": data['name'],
            "Difficulty": data['info']['difficulty'],
            "Tags": ",".join(data['tags']),
            "source": "riot",
            "scrape_date": today_str # <--- [å˜åŒ–2] ç»™æ•°æ®æ‰“ä¸Šæ—¶é—´æˆ³
        })
        target_champs.append({"name": data['name']})

    # å­˜å…¥ riot_stats è¡¨
    df_riot = pd.DataFrame(difficulty_list)
    
    # <--- [å˜åŒ–3] å…³é”®ä¿®æ”¹ï¼šæ”¹æˆ 'append' (è¿½åŠ æ¨¡å¼)
    df_riot.to_sql('riot_stats', conn, if_exists='append', index=False)
    print(f"âœ… Riot Data Appended! Total Champions: {len(df_riot)}")

except Exception as e:
    print(f"âŒ Riot Error: {e}")

# ==========================================
# ðŸ“º æ­¥éª¤ B: èŽ·å– Bilibili æ’­æ”¾æ•°æ® (å‰¯è¡¨)
# ==========================================
print("\nðŸ•µï¸â€â™‚ï¸ [2/2] Fetching Bilibili View Counts...")

# è¿™é‡Œçš„é€»è¾‘å’Œä¹‹å‰ä¸€æ ·ï¼Œæš‚æ—¶è·‘å‰10ä¸ªåšæµ‹è¯•
demo_champs = target_champs[:10] 

bili_stats = []
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Cookie": "buvid3=infoc;" 
}

for i, champ in enumerate(demo_champs):
    search_keyword = champ['name']
    print(f"   Searching {i+1}/{len(demo_champs)}: {search_keyword}...", end="\r")
    
    try:
        time.sleep(random.uniform(0.5, 1.5))
        
        url = "https://api.bilibili.com/x/web-interface/search/type"
        params = {
            "keyword": f"LOL {search_keyword}", 
            "search_type": "video", 
            "order": "click"
        }
        
        resp = requests.get(url, headers=headers, params=params, timeout=5)
        
        if resp.status_code == 200:
            data = resp.json()
            total_views = 0
            if data['code'] == 0 and 'result' in data['data']:
                video_list = data['data']['result']
                for v in video_list[:5]:
                    views = v.get('play') if 'play' in v else v.get('stat', {}).get('view', 0)
                    total_views += int(views)
            
            bili_stats.append({
                "Champion": champ['name'],
                "Bili_Top5_Views": total_views,
                "scrape_date": today_str # <--- [å˜åŒ–4] Bç«™æ•°æ®ä¹Ÿè¦æ‰“æ—¶é—´æˆ³
            })
            
    except Exception as e:
        print(f"\n   âš ï¸ Error fetching {search_keyword}: {e}")

# å­˜å…¥ bili_hot_champs è¡¨
if bili_stats:
    df_bili = pd.DataFrame(bili_stats)
    # <--- [å˜åŒ–5] å…³é”®ä¿®æ”¹ï¼šæ”¹æˆ 'append'
    df_bili.to_sql('bili_hot_champs', conn, if_exists='append', index=False)
    print(f"\nâœ… Bilibili Data Appended! Processed {len(df_bili)} champions.")
else:
    print("\nâš ï¸ No Bilibili data fetched.")

conn.close()
print("ðŸŽ‰ All Done! History preserved.")